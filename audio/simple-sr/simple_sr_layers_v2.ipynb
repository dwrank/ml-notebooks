{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b7f0a7-59d2-46fb-83b8-eb7514a357bd",
   "metadata": {},
   "source": [
    "# Explore the simple-sr model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c556b553-b7af-4de1-8563-7d2b16728dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 02:31:17.266046: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f18381-7fc4-43a7-ab54-73f48a9adf2e",
   "metadata": {},
   "source": [
    "## Import the saved model. Setup the paths and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97758ac4-a9ed-4e10-af68-f8a2627b7131",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['down', 'go', 'left', 'no', 'right', 'stop', 'up', 'yes']\n",
    "data_dir = pathlib.Path('data')\n",
    "imported = tf.saved_model.load('saved_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74419c9a-855d-4841-b6d3-fbad63539db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveform(wavfile):\n",
    "    x = tf.io.read_file(str(wavfile))\n",
    "    x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000)\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "\n",
    "def get_spectrogram(waveform):\n",
    "    # Convert the waveform to a spectrogram via a STFT\n",
    "    spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)\n",
    "\n",
    "    # Obtain the magnitude of the STFT\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "    # Add a 'channels' dimension, so that the spectrogram can be used as an\n",
    "    # image-like input data w/ convolution layers, which expect shape\n",
    "    # (batch_size, height, width, channels)\n",
    "    spectrogram = spectrogram[..., tf.newaxis]\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d0a65-336c-4244-9dc7-f9cc8fa7aa9c",
   "metadata": {},
   "source": [
    "## Get the spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0579a4a8-6c25-4cc8-886e-615942d3d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = get_waveform(data_dir/'yes.wav')\n",
    "spectrogram = get_spectrogram(waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb128d-c4bd-4180-963c-2dc689f8234b",
   "metadata": {},
   "source": [
    "## Load the model saved in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45b122ce-2b10-4639-877c-a4c31f33ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing_1 (Resizing)       (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      " normalization_4 (Normalizat  (None, 64, 64, 1)        3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 62, 62, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 31, 31, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               589952    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 683,659\n",
      "Trainable params: 683,656\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "h5_model = tf.keras.models.load_model('simple-sr_v2.h5')\n",
    "h5_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85f3b3-4514-420c-82d5-14bb00dc871f",
   "metadata": {},
   "source": [
    "## Test the HDF5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5549d84e-f3c5-4fb1-8f4b-e3be9467a6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=float32, numpy=\n",
       "array([[-2.6347573, -2.9110854,  2.067018 ,  1.7640504, -9.374807 ,\n",
       "        -4.109325 , -7.983361 , 10.585677 ]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = spectrogram[tf.newaxis,...]\n",
    "result = h5_model(input_data, training=False)\n",
    "label = label_names[result.numpy().argmax()]\n",
    "print('Prediction:', label)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4f49e3-c93e-408a-8e76-b3e86224f87d",
   "metadata": {},
   "source": [
    "## Explore the model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2c0be46-af37-4c23-bab7-d4257d7bb29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.preprocessing.image_preprocessing.Resizing at 0x1381b5190>,\n",
       " <keras.layers.preprocessing.normalization.Normalization at 0x137b2b490>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x138179a10>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x138f03290>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x138ece7d0>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x138edfe10>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x138edc650>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x138ee2710>,\n",
       " <keras.layers.regularization.dropout.Dropout at 0x138ee38d0>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x138ebfd90>,\n",
       " <keras.layers.core.dense.Dense at 0x138ebb790>,\n",
       " <keras.layers.regularization.dropout.Dropout at 0x138f07b90>,\n",
       " <keras.layers.core.dense.Dense at 0x138f07510>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38840a5f-a4ef-49fa-b85b-b6804ff6e9ec",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58e0f4e0-a845-41ab-bf16-b746f0d707dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 124, 129, 1]),\n",
       " array([0.00087561, 0.00134371, 0.00557508, 0.01203688, 0.01582851,\n",
       "        0.01979508, 0.03313684, 0.05369601, 0.05009932, 0.03737277,\n",
       "        0.0312091 , 0.00657593, 0.00977934, 0.00414987, 0.01013514,\n",
       "        0.03394571, 0.03811777, 0.05212038, 0.07652813, 0.09006108,\n",
       "        0.06934591, 0.09528538, 0.09866577, 0.05166683, 0.03728538,\n",
       "        0.02647714, 0.03595338, 0.03264239, 0.03675836, 0.05252869,\n",
       "        0.05268585, 0.03330975], dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape, input_data[0,0,:32,:].numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da319312-f159-45f9-bc2a-13c9be768d6b",
   "metadata": {},
   "source": [
    "## Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "286ef09c-78b2-4808-897b-51ee28ea31ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 64, 64, 1]),\n",
       " array([0.00155563, 0.01450971, 0.01721152, 0.03461045, 0.04996715,\n",
       "        0.06476526, 0.07763705, 0.04730197, 0.05492217, 0.07322476,\n",
       "        0.09561469, 0.05896866, 0.04744271, 0.02823398, 0.06757322,\n",
       "        0.05356693, 0.0716002 , 0.08599924, 0.10384966, 0.08202535,\n",
       "        0.07202959, 0.06213472, 0.04379842, 0.02111142, 0.02182574,\n",
       "        0.02863349, 0.05733498, 0.02389398, 0.05709929, 0.02924848,\n",
       "        0.04101247, 0.05370279], dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resize_layer = h5_model.layers[0]\n",
    "resize_out = resize_layer(input_data)\n",
    "resize_out.shape, resize_out[0,0,:32,:].numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e720b74-2397-4e31-a25d-6bf3ff695364",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "867aefdc-7ee7-48e9-9d1e-7f910cfc14aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 64, 64, 1]),\n",
       " array([-0.16206573, -0.145115  , -0.14157963, -0.1188127 , -0.09871808,\n",
       "        -0.07935438, -0.06251133, -0.10220551, -0.0922343 , -0.06828492,\n",
       "        -0.03898715, -0.08693938, -0.10202137, -0.12715647, -0.0756801 ,\n",
       "        -0.09400767], dtype=float32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_layer = h5_model.layers[1]\n",
    "norm_out = norm_layer(resize_out.numpy())\n",
    "norm_out.shape, norm_out[0,0,:16,:].numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925a543-f099-43d7-917a-42860475f335",
   "metadata": {},
   "source": [
    "## Conv2D 32 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b32316c-adf7-4f94-9600-e6442233196f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 62, 62, 32]),\n",
       " array([0.00627902, 0.        , 0.02095808, 0.01433451, 0.        ,\n",
       "        0.00889511, 0.0643165 , 0.03762578, 0.        , 0.02074122,\n",
       "        0.00472939, 0.        , 0.01017278, 0.00777538, 0.01832406,\n",
       "        0.        , 0.        , 0.01559877, 0.01055296, 0.00048798,\n",
       "        0.03145018, 0.01437424, 0.00606427, 0.00207761, 0.01075713,\n",
       "        0.02302407, 0.00345798, 0.00418103, 0.        , 0.00154825,\n",
       "        0.03864209, 0.02948506], dtype=float32))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_32_layer = h5_model.layers[2]\n",
    "conv2d_32_out = conv2d_32_layer(norm_out.numpy())\n",
    "conv2d_32_out.shape, conv2d_32_out[0,0,0,:32].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b530adc-24e0-4a51-af24-1eef7f0fb158",
   "metadata": {},
   "source": [
    "## MaxPooling2D 32 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "840a49b1-6e18-4f99-8038-08962b224ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 31, 31, 32]),\n",
       " array([0.01072034, 0.        , 0.02095808, 0.02108153, 0.        ,\n",
       "        0.01503734, 0.0643165 , 0.03762578, 0.        , 0.0217897 ,\n",
       "        0.00606663, 0.        , 0.02001369, 0.00966259, 0.03384773,\n",
       "        0.        , 0.        , 0.03154362, 0.01652316, 0.01577898,\n",
       "        0.03238086, 0.03042345, 0.02748973, 0.01532979, 0.0124368 ,\n",
       "        0.0242172 , 0.01190388, 0.01375844, 0.        , 0.00669544,\n",
       "        0.0389999 , 0.02948506], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_32_layer = h5_model.layers[3]\n",
    "mp_32_out = mp_32_layer(conv2d_32_out.numpy())\n",
    "mp_32_out.shape, mp_32_out[0,0,0,:32].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508ffb7-ffcf-4c88-a754-5a59e4d49f9d",
   "metadata": {},
   "source": [
    "## Conv2D 64 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58561114-51bb-4585-a771-c186d0d5ecd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 29, 29, 64]),\n",
       " array([0.24887821, 0.26164997, 0.        , 0.        , 0.29989332,\n",
       "        0.2574429 , 0.        , 0.        , 0.        , 0.11881153,\n",
       "        0.        , 0.15549089, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34623715, 0.26356146, 0.        , 0.32560804,\n",
       "        0.48643997, 0.        , 0.11214834, 0.        , 0.14620453,\n",
       "        0.        , 0.        , 0.2733803 , 0.4414402 , 0.37427112,\n",
       "        0.        , 0.23532963], dtype=float32))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_64_layer = h5_model.layers[4]\n",
    "conv2d_64_out = conv2d_64_layer(mp_32_out.numpy())\n",
    "conv2d_64_out.shape, conv2d_64_out[0,0,0,:32].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c4a358-fc77-44c3-8b28-82ee0d47cf2c",
   "metadata": {},
   "source": [
    "## MaxPooling2D 64 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bd1d9da-8a24-42c6-b77a-d0ffb20ba613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 14, 14, 64]),\n",
       " array([0.3395589 , 0.36666068, 0.12057038, 0.17979348, 0.5102749 ,\n",
       "        0.2574429 , 0.        , 0.        , 0.05522262, 0.11881153,\n",
       "        0.        , 0.15549089, 0.        , 0.19072533, 0.        ,\n",
       "        0.        , 0.34623715, 0.26356146, 0.29949617, 0.32560804,\n",
       "        0.48643997, 0.09932759, 0.2327676 , 0.        , 0.3357129 ,\n",
       "        0.        , 0.        , 0.2733803 , 0.55575466, 0.37427112,\n",
       "        0.        , 0.23532963], dtype=float32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_64_layer = h5_model.layers[5]\n",
    "mp_64_out = mp_64_layer(conv2d_64_out.numpy())\n",
    "mp_64_out.shape, mp_64_out[0,0,0,:32].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df09936-290d-438e-9626-0aeec89dbec1",
   "metadata": {},
   "source": [
    "## Conv2D 128 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6d1ca55-d5ff-46bf-b7d1-ab7910734b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 12, 12, 128]),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00907242, 0.        , 0.        ,\n",
       "        0.        , 0.25049394, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.10954612, 0.        , 0.        ,\n",
       "        0.        , 0.1922076 , 0.        , 0.65980977, 0.        ,\n",
       "        0.        , 0.        ], dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_128_layer = h5_model.layers[6]\n",
    "conv2d_128_out = conv2d_128_layer(mp_64_out.numpy())\n",
    "conv2d_128_out.shape, conv2d_128_out[0,0,0,:32].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620dd37-6278-4c91-9ce0-789f61d1de5e",
   "metadata": {},
   "source": [
    "## MaxPooling2D 128 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a81607c7-6530-450d-95ea-32bc4f1e2452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 6, 6, 128]),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05285494,\n",
       "        0.        , 0.        , 0.02493029, 0.09165753, 0.        ,\n",
       "        0.        , 0.25049394, 0.15503892, 0.05853331, 0.        ,\n",
       "        0.11574493, 0.        , 0.10954612, 0.        , 0.        ,\n",
       "        0.        , 0.23547164, 0.        , 0.65980977, 0.        ,\n",
       "        0.        , 0.        ], dtype=float32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_128_layer = h5_model.layers[7]\n",
    "mp_128_out = mp_128_layer(conv2d_128_out.numpy())\n",
    "mp_128_out.shape, mp_128_out[0,0,0,:32].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95df10-5316-491c-982a-4544b27ad2b9",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de8cd9e1-654d-4f17-9736-eaf02096b727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 6, 6, 128]),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05285494,\n",
       "        0.        , 0.        , 0.02493029, 0.09165753, 0.        ,\n",
       "        0.        , 0.25049394, 0.15503892, 0.05853331, 0.        ,\n",
       "        0.11574493, 0.        , 0.10954612, 0.        , 0.        ,\n",
       "        0.        , 0.23547164, 0.        , 0.65980977, 0.        ,\n",
       "        0.        , 0.        ], dtype=float32))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer = h5_model.layers[8]\n",
    "dropout_out = dropout_layer(mp_128_out.numpy())\n",
    "dropout_out.shape, dropout_out[0,0,0,:32].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14b69d-33b4-4fbf-bd91-15a4b0cc9c83",
   "metadata": {},
   "source": [
    "## Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a270a0e-cc71-48b3-b6c2-99cc84e584c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 4608]),\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05285494,\n",
       "        0.        , 0.        , 0.02493029, 0.09165753, 0.        ,\n",
       "        0.        , 0.25049394, 0.15503892, 0.05853331, 0.        ,\n",
       "        0.11574493, 0.        , 0.10954612, 0.        , 0.        ,\n",
       "        0.        , 0.23547164, 0.        , 0.65980977, 0.        ,\n",
       "        0.        , 0.        ], dtype=float32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_layer = h5_model.layers[9]\n",
    "flatten_out = flatten_layer(dropout_out.numpy())\n",
    "flatten_out.shape, flatten_out[0,:32].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c1f13-1829-4ddd-aebd-e15f161d0413",
   "metadata": {},
   "source": [
    "## Dense 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "922a71fa-be16-4bf4-b054-c6d321121ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 128]),\n",
       " array([0.        , 3.2486835 , 0.        , 4.235691  , 3.8311493 ,\n",
       "        3.3287628 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.2558393 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.1135519 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.03024962, 0.846521  , 0.        ,\n",
       "        1.3403792 , 0.289591  , 2.0219226 , 0.        , 0.2902855 ,\n",
       "        0.        , 0.11955655], dtype=float32))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_128_layer = h5_model.layers[10]\n",
    "d_128_out = d_128_layer(flatten_out.numpy())\n",
    "d_128_out.shape, d_128_out[0,:32].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257c7b7-5917-4a1f-903d-0f78504937c0",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36a10966-64f8-4251-b9fe-42622c1148ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 128]),\n",
       " array([0.        , 3.2486835 , 0.        , 4.235691  , 3.8311493 ,\n",
       "        3.3287628 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.2558393 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.1135519 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.03024962, 0.846521  , 0.        ,\n",
       "        1.3403792 , 0.289591  , 2.0219226 , 0.        , 0.2902855 ,\n",
       "        0.        , 0.11955655], dtype=float32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer = h5_model.layers[11]\n",
    "dropout_out = dropout_layer(d_128_out.numpy())\n",
    "dropout_out.shape, dropout_out[0,:32].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc4bab-9fa9-454f-a27c-daa5c04fb376",
   "metadata": {},
   "source": [
    "## Dense 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07489953-cf78-4b92-893b-ab29b19f339f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 8]),\n",
       " array([[-2.6347573, -2.9110854,  2.067018 ,  1.7640504, -9.374807 ,\n",
       "         -4.109325 , -7.983361 , 10.585677 ]], dtype=float32))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_8_layer = h5_model.layers[12]\n",
    "d_8_out = d_8_layer(dropout_out.numpy())\n",
    "d_8_out.shape, d_8_out.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a95ad07-c446-409b-8346-ec8ff58908d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: yes\n"
     ]
    }
   ],
   "source": [
    "label = label_names[d_8_out.numpy().argmax()]\n",
    "print('Prediction:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb53918-95ec-47a3-975c-ee02f5b64623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
