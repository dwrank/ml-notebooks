{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3fe264-3247-405b-96fa-531abf4e4fad",
   "metadata": {},
   "source": [
    "# Implement Resizing and Normalization Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "078af610-e416-498b-9afe-5dad9dd2078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6229b6c3-1970-434f-b22f-1d1fa0b82220",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "502936fe-e22c-4c53-9e23-4af9df24c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveform(wavfile):\n",
    "    x = tf.io.read_file(str(wavfile))\n",
    "    x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000)\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "\n",
    "def get_spectrogram(waveform):\n",
    "    # Convert the waveform to a spectrogram via a STFT\n",
    "    spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)\n",
    "\n",
    "    # Obtain the magnitude of the STFT\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "    # Add a 'channels' dimension, so that the spectrogram can be used as an\n",
    "    # image-like input data w/ convolution layers, which expect shape\n",
    "    # (batch_size, height, width, channels)\n",
    "    spectrogram = spectrogram[..., tf.newaxis]\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d047638-9b4e-4ad9-aed0-4489da74a1cd",
   "metadata": {},
   "source": [
    "## Get the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48c928d6-ee63-4803-aed3-921ca8e85291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 124, 129, 1]),\n",
       " <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       " array([[0.00021577],\n",
       "        [0.00242205],\n",
       "        [0.00827248],\n",
       "        [0.01238501],\n",
       "        [0.01051275],\n",
       "        [0.01144113],\n",
       "        [0.01872324],\n",
       "        [0.02427843],\n",
       "        [0.04116756],\n",
       "        [0.09470174]], dtype=float32)>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform = get_waveform(data_dir/'test/h_yes.wav')\n",
    "spec = get_spectrogram(waveform)\n",
    "input_data = spec[tf.newaxis,...]\n",
    "input_data.shape, input_data[0,0,:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ce07a-1be8-447f-b225-6d9ef8bebc4e",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34b59714-c327-44cd-a908-553cd3baa6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing (Resizing)         (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 32, 32, 1)        3         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1605760   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,625,611\n",
      "Trainable params: 1,625,608\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "h5_model = tf.keras.models.load_model('simple_audio.h5')\n",
    "h5_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ace709-9874-4c76-9316-c1908fb09776",
   "metadata": {},
   "source": [
    "## Explore the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c25c8f0-ed33-4c8f-a966-757f3657b665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.preprocessing.image_preprocessing.Resizing at 0x10b38bc90>,\n",
       " <keras.layers.preprocessing.normalization.Normalization at 0x12d0d7690>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x12d5d7090>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x12d524250>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x12d591250>,\n",
       " <keras.layers.regularization.dropout.Dropout at 0x12d6bf790>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x12d6bec50>,\n",
       " <keras.layers.core.dense.Dense at 0x12d6bd2d0>,\n",
       " <keras.layers.regularization.dropout.Dropout at 0x12d6c3ed0>,\n",
       " <keras.layers.core.dense.Dense at 0x12d6c38d0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee54144b-bc75-4526-aa78-63ffaf5c1979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1, 124, 129, 1) dtype=float32 (created by layer 'input_2')>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Input(shape=input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dccd255-e8cc-450a-8ca0-75b4e2ca760e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 32, 32, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample the input\n",
    "l1 = layers.Resizing(32, 32)(input_data)\n",
    "l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2c4b40a-1bc6-445d-9b37-c999ec211d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 32, 32, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize\n",
    "norm_layer = h5_model.layers[1]\n",
    "l2 = norm_layer(l1.numpy())  # tf.convert_to_tensor(l1 / np.linalg.norm(l1.numpy()), dtype=tf.float32)\n",
    "l2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180634a0-28a8-4907-8083-c4bffa0fdcac",
   "metadata": {},
   "source": [
    "## Implement Resizing (bilinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07ef1295-6dfd-4924-901f-a11480f631f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ported from tensorflow/lite/kernels/internal/reference/resize_bilinear.h ComputeInterpolationValues\n",
    "def compute_interpolation_values(value, scale, input_size, half_pixel_centers=True):\n",
    "    if half_pixel_centers:\n",
    "        scaled_value = (value + 0.5) * scale - 0.5\n",
    "    else:\n",
    "        scaled_value = value * scale\n",
    "\n",
    "    scaled_value_floor = float(math.floor(scaled_value))\n",
    "    lower_bound = int(max(scaled_value_floor, 0))\n",
    "    upper_bound =  int(min(math.ceil(scaled_value), input_size - 1))\n",
    "\n",
    "    return scaled_value, lower_bound, upper_bound\n",
    "\n",
    "\n",
    "# ported from tensorflow/lite/kernels/internal/reference/resize_bilinear.h ResizeBilinear\n",
    "def resize_bilinear(input_data, output_width, output_height, align_corners=False):\n",
    "    output_data = np.zeros(input_data.shape, dtype=input_data.dtype)\n",
    "\n",
    "    batches, input_height, input_width, depth = input_data.shape\n",
    "\n",
    "    if align_corners and output_height > 1:\n",
    "        height_scale = (input_height - 1) / (output_height - 1)\n",
    "    else:\n",
    "        height_scale = input_height / output_height\n",
    "\n",
    "    if align_corners and output_width > 1:\n",
    "        width_scale = (input_width - 1) / (output_width - 1)\n",
    "    else:\n",
    "        width_scale = input_width / output_width\n",
    "\n",
    "    if 'int' in input_data.dtype.name:\n",
    "        rounding_offset = 0.5\n",
    "    else:\n",
    "        rounding_offset = 0.0\n",
    "\n",
    "    for b in range(batches):\n",
    "        for y in range(output_height):\n",
    "            input_y, y0, y1 = compute_interpolation_values(y, height_scale, input_height)\n",
    "\n",
    "            for x in range(output_width):\n",
    "                input_x, x0, x1 = compute_interpolation_values(x, width_scale, input_width)\n",
    "\n",
    "                for c in range(depth):\n",
    "                    interpolation = input_data[b, y0, x0, c] * (1 - (input_y - y0)) * (1 - (input_x - x0)) + \\\n",
    "                                    input_data[b, y1, x0, c] * (input_y - y0) * (1 - (input_x - x0)) + \\\n",
    "                                    input_data[b, y0, x1, c] * (1 - (input_y - y0)) * (input_x - x0) + \\\n",
    "                                    input_data[b, y1, x1, c] * (input_y - y0) * (input_x - x0) + \\\n",
    "                                    rounding_offset\n",
    "                    output_data[b, y, x, c] = interpolation\n",
    "                    #if y == 0 and x < 10: print('Interp %f %d %d, %f %d %d' % (input_y, y0, y1, input_x, x0, x1))\n",
    "\n",
    "    return np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dd2e36a-6caf-477f-9b97-53b026b1487f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 124, 129, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cd430fe-bab7-4916-abbf-68686931a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interp 1.437500 1 2, 1.515625 1 2\n",
      "Interp 1.437500 1 2, 5.546875 5 6\n",
      "Interp 1.437500 1 2, 9.578125 9 10\n",
      "Interp 1.437500 1 2, 13.609375 13 14\n",
      "Interp 1.437500 1 2, 17.640625 17 18\n",
      "Interp 1.437500 1 2, 21.671875 21 22\n",
      "Interp 1.437500 1 2, 25.703125 25 26\n",
      "Interp 1.437500 1 2, 29.734375 29 30\n",
      "Interp 1.437500 1 2, 33.765625 33 34\n",
      "Interp 1.437500 1 2, 37.796875 37 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00331012],\n",
       "       [0.0239757 ],\n",
       "       [0.09051986],\n",
       "       [0.05873445],\n",
       "       [0.07851263],\n",
       "       [0.05929786],\n",
       "       [0.05793667],\n",
       "       [0.02979692],\n",
       "       [0.03618773],\n",
       "       [0.02978715]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data = input_data.numpy()\n",
    "resized = resize_bilinear(np_data, 32, 32)\n",
    "resized[0,0,:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d2a6e8a-04b4-4531-bfec-d587052d5fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       "array([[0.00331012],\n",
       "       [0.0239757 ],\n",
       "       [0.09051986],\n",
       "       [0.05873445],\n",
       "       [0.07851262],\n",
       "       [0.05929786],\n",
       "       [0.05793667],\n",
       "       [0.02979692],\n",
       "       [0.03618773],\n",
       "       [0.02978715]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify with the source of truth\n",
    "l1[0,0,:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af71dae-8da6-4339-9a3a-de783626aa3f",
   "metadata": {},
   "source": [
    "## Explore the normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bf868c5-de4e-40a6-8209-ed59bb67242d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[0.12540944]]]], dtype=float32),\n",
       " array([[[[0.58403146]]]], dtype=float32),\n",
       " 102374400)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_layer.mean.numpy(), norm_layer.variance.numpy(), norm_layer.count.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4c22b32-9942-4256-8b14-8bd4f54544ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 32, 32, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_output = norm_layer(l1.numpy())\n",
    "norm_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9835d29-92c2-438c-aa00-8f22f6e0c71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       "array([[-0.15976997],\n",
       "       [-0.13272853],\n",
       "       [-0.04565387],\n",
       "       [-0.08724585],\n",
       "       [-0.06136563],\n",
       "       [-0.08650862],\n",
       "       [-0.08828977],\n",
       "       [-0.12511134],\n",
       "       [-0.11674879],\n",
       "       [-0.12512411]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_output[0,0,:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851c708-75a1-4ddf-9d24-6ed0c362d638",
   "metadata": {},
   "source": [
    "## Verify creating a new normalization layer with provided mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a5889eb-f121-4e1f-8383-262e581ec9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 32, 32, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the normalization layer\n",
    "mean = 0.12540944\n",
    "variance = 0.58403146\n",
    "nl = layers.Normalization(mean=mean, variance=variance)\n",
    "nl_output = norm_layer(l1.numpy())\n",
    "nl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2932a811-6ff2-4d72-9a4a-ff856159f6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       "array([[-0.15976997],\n",
       "       [-0.13272853],\n",
       "       [-0.04565387],\n",
       "       [-0.08724585],\n",
       "       [-0.06136563],\n",
       "       [-0.08650862],\n",
       "       [-0.08828977],\n",
       "       [-0.12511134],\n",
       "       [-0.11674879],\n",
       "       [-0.12512411]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_output[0,0,:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85ee23-8141-4029-9220-356e296ed343",
   "metadata": {},
   "source": [
    "## Do the normalization calculation with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "128196d6-8c44-48f5-99de-770174fa83ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15976997],\n",
       "       [-0.13272853],\n",
       "       [-0.04565387],\n",
       "       [-0.08724585],\n",
       "       [-0.06136563],\n",
       "       [-0.08650862],\n",
       "       [-0.08828977],\n",
       "       [-0.12511134],\n",
       "       [-0.11674879],\n",
       "       [-0.12512411]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npnl_output = (l1.numpy() - mean) / math.sqrt(variance)\n",
    "npnl_output[0,0,:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a1caa-59b2-43e1-9c3a-06e4aa0f658f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
